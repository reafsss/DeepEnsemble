{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Uncertainty Baselines Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"CIFAR{10,100} dataset builders.\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from robustness_metrics.common import types\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from uncertainty_baselines.datasets import augment_utils\n",
    "from uncertainty_baselines.datasets import augmix\n",
    "from uncertainty_baselines.datasets import base\n",
    "\n",
    "# We use the convention of using mean = np.mean(train_images, axis=(0,1,2))\n",
    "# and std = np.std(train_images, axis=(0,1,2)).\n",
    "CIFAR10_MEAN = np.array([0.4914, 0.4822, 0.4465])\n",
    "CIFAR10_STD = np.array([0.2470, 0.2435, 0.2616])\n",
    "# Previously we used std = np.mean(np.std(train_images, axis=(1, 2)), axis=0)\n",
    "# which gave std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype), however\n",
    "# we change convention to use the std over the entire training set instead.\n",
    "\n",
    "\n",
    "def _tuple_dict_fn_converter(fn, *args):\n",
    "\n",
    "  def dict_fn(batch_dict):\n",
    "    images, labels = fn(*args, batch_dict['features'], batch_dict['labels'])\n",
    "    return {'features': images, 'labels': labels}\n",
    "\n",
    "  return dict_fn\n",
    "\n",
    "\n",
    "class _CifarDataset(base.BaseDataset):\n",
    "  \"\"\"CIFAR dataset builder abstract class.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      name: str,\n",
    "      fingerprint_key: str,\n",
    "      split: str,\n",
    "      seed: Optional[Union[int, tf.Tensor]] = None,\n",
    "      validation_percent: float = 0.0,\n",
    "      shuffle_buffer_size: Optional[int] = None,\n",
    "      num_parallel_parser_calls: int = 64,\n",
    "      drop_remainder: bool = True,\n",
    "      normalize: bool = True,\n",
    "      try_gcs: bool = False,\n",
    "      download_data: bool = False,\n",
    "      use_bfloat16: bool = False,\n",
    "      aug_params: Optional[Dict[str, Any]] = None,\n",
    "      data_dir: Optional[str] = None,\n",
    "      is_training: Optional[bool] = None,\n",
    "      **unused_kwargs: Dict[str, Any]):\n",
    "    \"\"\"Create a CIFAR10 or CIFAR100 tf.data.Dataset builder.\n",
    "    Args:\n",
    "      name: the name of this dataset, either 'cifar10' or 'cifar100'.\n",
    "      fingerprint_key: The name of the feature holding a string that will be\n",
    "        used to create an element id using a fingerprinting function. If None,\n",
    "        then `ds.enumerate()` is added before the `ds.map(preprocessing_fn)` is\n",
    "        called and an `id` field is added to the example Dict.\n",
    "      split: a dataset split, either a custom tfds.Split or one of the\n",
    "        tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n",
    "        names.\n",
    "      seed: the seed used as a source of randomness.\n",
    "      validation_percent: the percent of the training set to use as a validation\n",
    "        set.\n",
    "      shuffle_buffer_size: the number of example to use in the shuffle buffer\n",
    "        for tf.data.Dataset.shuffle().\n",
    "      num_parallel_parser_calls: the number of parallel threads to use while\n",
    "        preprocessing in tf.data.Dataset.map().\n",
    "      drop_remainder: whether or not to drop the last batch of data if the\n",
    "        number of points is not exactly equal to the batch size. This option\n",
    "        needs to be True for running on TPUs.\n",
    "      normalize: whether or not to normalize each image by the CIFAR dataset\n",
    "        mean and stddev.\n",
    "      try_gcs: Whether or not to try to use the GCS stored versions of dataset\n",
    "        files.\n",
    "      download_data: Whether or not to download data before loading.\n",
    "      use_bfloat16: Whether or not to load the data in bfloat16 or float32.\n",
    "      aug_params: hyperparameters for the data augmentation pre-processing.\n",
    "      data_dir: Directory to read/write data, that is passed to the\n",
    "        tfds dataset_builder as a data_dir parameter.\n",
    "      is_training: Whether or not the given `split` is the training split. Only\n",
    "        required when the passed split is not one of ['train', 'validation',\n",
    "        'test', tfds.Split.TRAIN, tfds.Split.VALIDATION, tfds.Split.TEST].\n",
    "    \"\"\"\n",
    "    self._normalize = normalize\n",
    "    dataset_builder = tfds.builder(\n",
    "        name, try_gcs=try_gcs,\n",
    "        data_dir=data_dir)\n",
    "    if is_training is None:\n",
    "      is_training = split in ['train', tfds.Split.TRAIN]\n",
    "    new_split = base.get_validation_percent_split(\n",
    "        dataset_builder, validation_percent, split)\n",
    "    super(_CifarDataset, self).__init__(\n",
    "        name=name,\n",
    "        dataset_builder=dataset_builder,\n",
    "        split=new_split,\n",
    "        seed=seed,\n",
    "        is_training=is_training,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        num_parallel_parser_calls=num_parallel_parser_calls,\n",
    "        drop_remainder=drop_remainder,\n",
    "        fingerprint_key=fingerprint_key,\n",
    "        download_data=download_data,\n",
    "        cache=True)\n",
    "\n",
    "    self._use_bfloat16 = use_bfloat16\n",
    "    if aug_params is None:\n",
    "      aug_params = {}\n",
    "    self._adaptive_mixup = aug_params.get('adaptive_mixup', False)\n",
    "    ensemble_size = aug_params.get('ensemble_size', 1)\n",
    "    if self._adaptive_mixup and 'mixup_coeff' not in aug_params:\n",
    "      # Hard target in the first epoch!\n",
    "      aug_params['mixup_coeff'] = tf.ones([ensemble_size, 10])\n",
    "    self._aug_params = aug_params\n",
    "\n",
    "  def _create_process_example_fn(self) -> base.PreProcessFn:\n",
    "\n",
    "    def _example_parser(example: types.Features) -> types.Features:\n",
    "      \"\"\"A pre-process function to return images in [0, 1].\"\"\"\n",
    "      image = example['image']\n",
    "      image_dtype = tf.bfloat16 if self._use_bfloat16 else tf.float32\n",
    "      use_augmix = self._aug_params.get('augmix', False)\n",
    "      if self._is_training:\n",
    "        image_shape = tf.shape(image)\n",
    "        # Expand the image by 2 pixels, then crop back down to 32x32.\n",
    "        image = tf.image.resize_with_crop_or_pad(\n",
    "            image, image_shape[0] + 4, image_shape[1] + 4)\n",
    "        # Note that self._seed will already be shape (2,), as is required for\n",
    "        # stateless random ops, and so will per_example_step_seed.\n",
    "        per_example_step_seed = tf.random.experimental.stateless_fold_in(\n",
    "            self._seed, example[self._enumerate_id_key])\n",
    "        # per_example_step_seeds will be of size (num, 3).\n",
    "        # First for random_crop, second for flip, third optionally for\n",
    "        # RandAugment, and foruth optionally for Augmix.\n",
    "        per_example_step_seeds = tf.random.experimental.stateless_split(\n",
    "            per_example_step_seed, num=4)\n",
    "        image = tf.image.stateless_random_crop(\n",
    "            image,\n",
    "            (image_shape[0], image_shape[0], 3),\n",
    "            seed=per_example_step_seeds[0])\n",
    "        image = tf.image.stateless_random_flip_left_right(\n",
    "            image,\n",
    "            seed=per_example_step_seeds[1])\n",
    "\n",
    "        # Only random augment for now.\n",
    "        if self._aug_params.get('random_augment', False):\n",
    "          count = self._aug_params['aug_count']\n",
    "          augment_seeds = tf.random.experimental.stateless_split(\n",
    "              per_example_step_seeds[2], num=count)\n",
    "          augmenter = augment_utils.RandAugment()\n",
    "          augmented = [\n",
    "              augmenter.distort(image, seed=augment_seeds[c])\n",
    "              for c in range(count)\n",
    "          ]\n",
    "          image = tf.stack(augmented)\n",
    "\n",
    "        if use_augmix:\n",
    "          augmenter = augment_utils.RandAugment()\n",
    "          image = augmix.do_augmix(\n",
    "              image, self._aug_params, augmenter, image_dtype,\n",
    "              mean=CIFAR10_MEAN, std=CIFAR10_STD,\n",
    "              seed=per_example_step_seeds[3])\n",
    "\n",
    "      # The image has values in the range [0, 1].\n",
    "      # Optionally normalize by the dataset statistics.\n",
    "      if not use_augmix:\n",
    "        if self._normalize:\n",
    "          image = augmix.normalize_convert_image(\n",
    "              image, image_dtype, mean=CIFAR10_MEAN, std=CIFAR10_STD)\n",
    "        else:\n",
    "          image = tf.image.convert_image_dtype(image, image_dtype)\n",
    "      parsed_example = example.copy()\n",
    "      parsed_example['features'] = image\n",
    "\n",
    "      # Note that labels are always float32, even when images are bfloat16.\n",
    "      mixup_alpha = self._aug_params.get('mixup_alpha', 0)\n",
    "      label_smoothing = self._aug_params.get('label_smoothing', 0.)\n",
    "      should_onehot = mixup_alpha > 0 or label_smoothing > 0\n",
    "      if should_onehot:\n",
    "        parsed_example['labels'] = tf.one_hot(\n",
    "            example['label'], 10, dtype=tf.float32)\n",
    "      else:\n",
    "        parsed_example['labels'] = tf.cast(example['label'], tf.float32)\n",
    "\n",
    "      del parsed_example['image']\n",
    "      del parsed_example['label']\n",
    "      return parsed_example\n",
    "\n",
    "    return _example_parser\n",
    "\n",
    "  def _create_process_batch_fn(\n",
    "      self,\n",
    "      batch_size: int) -> Optional[base.PreProcessFn]:\n",
    "    if self._is_training and self._aug_params.get('mixup_alpha', 0) > 0:\n",
    "      if self._adaptive_mixup:\n",
    "        return _tuple_dict_fn_converter(\n",
    "            augmix.adaptive_mixup, batch_size, self._aug_params)\n",
    "      else:\n",
    "        return _tuple_dict_fn_converter(\n",
    "            augmix.mixup, batch_size, self._aug_params)\n",
    "    return None\n",
    "\n",
    "\n",
    "class Cifar10Dataset(_CifarDataset):\n",
    "  \"\"\"CIFAR10 dataset builder class.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(Cifar10Dataset, self).__init__(\n",
    "        name='cifar10',\n",
    "        fingerprint_key='id',\n",
    "        **kwargs)\n",
    "\n",
    "\n",
    "class Cifar100Dataset(_CifarDataset):\n",
    "  \"\"\"CIFAR100 dataset builder class.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(Cifar100Dataset, self).__init__(\n",
    "        name='cifar100',\n",
    "        fingerprint_key='id',\n",
    "        **kwargs)\n",
    "\n",
    "\n",
    "class Cifar10CorruptedDataset(_CifarDataset):\n",
    "  \"\"\"CIFAR10-C dataset builder class.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      corruption_type: str,\n",
    "      severity: int,\n",
    "      **kwargs):\n",
    "    \"\"\"Create a CIFAR10-C tf.data.Dataset builder.\n",
    "    Args:\n",
    "      corruption_type: Corruption name.\n",
    "      severity: Corruption severity, an integer between 1 and 5.\n",
    "      **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    super(Cifar10CorruptedDataset, self).__init__(\n",
    "        name=f'cifar10_corrupted/{corruption_type}_{severity}',\n",
    "        fingerprint_key=None,\n",
    "        **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
