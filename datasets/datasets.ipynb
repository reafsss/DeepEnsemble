{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f57e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Uncertainty Baselines Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Dataset getter utility.\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from uncertainty_baselines.datasets.base import BaseDataset\n",
    "from uncertainty_baselines.datasets.cifar import Cifar100Dataset\n",
    "from uncertainty_baselines.datasets.cifar import Cifar10CorruptedDataset\n",
    "from uncertainty_baselines.datasets.cifar import Cifar10Dataset\n",
    "from uncertainty_baselines.datasets.cifar100_corrupted import Cifar100CorruptedDataset\n",
    "from uncertainty_baselines.datasets.clinc_intent import ClincIntentDetectionDataset\n",
    "from uncertainty_baselines.datasets.criteo import CriteoDataset\n",
    "from uncertainty_baselines.datasets.diabetic_retinopathy_detection import DiabeticRetinopathyDetectionDataset\n",
    "from uncertainty_baselines.datasets.genomics_ood import GenomicsOodDataset\n",
    "from uncertainty_baselines.datasets.glue import GlueDatasets\n",
    "from uncertainty_baselines.datasets.imagenet import ImageNetDataset\n",
    "from uncertainty_baselines.datasets.mnist import MnistDataset\n",
    "from uncertainty_baselines.datasets.mnli import MnliDataset\n",
    "from uncertainty_baselines.datasets.movielens import MovieLensDataset\n",
    "from uncertainty_baselines.datasets.places import Places365Dataset\n",
    "from uncertainty_baselines.datasets.random import RandomGaussianImageDataset\n",
    "from uncertainty_baselines.datasets.random import RandomRademacherImageDataset\n",
    "from uncertainty_baselines.datasets.svhn import SvhnDataset\n",
    "from uncertainty_baselines.datasets.toxic_comments import CivilCommentsDataset\n",
    "from uncertainty_baselines.datasets.toxic_comments import CivilCommentsIdentitiesDataset\n",
    "from uncertainty_baselines.datasets.toxic_comments import WikipediaToxicityDataset\n",
    "\n",
    "try:\n",
    "  from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
    "except ImportError as e:\n",
    "  warnings.warn(f'Skipped due to ImportError: {e}')\n",
    "  SpeechCommandsDataset = None\n",
    "\n",
    "DATASETS = {\n",
    "    'cifar100': Cifar100Dataset,\n",
    "    'cifar10': Cifar10Dataset,\n",
    "    'cifar10_corrupted': Cifar10CorruptedDataset,\n",
    "    'cifar100_corrupted': Cifar100CorruptedDataset,\n",
    "    'civil_comments': CivilCommentsDataset,\n",
    "    'civil_comments_identities': CivilCommentsIdentitiesDataset,\n",
    "    'clinic_intent': ClincIntentDetectionDataset,\n",
    "    'criteo': CriteoDataset,\n",
    "    'diabetic_retinopathy_detection': DiabeticRetinopathyDetectionDataset,\n",
    "    'imagenet': ImageNetDataset,\n",
    "    'mnist': MnistDataset,\n",
    "    'mnli': MnliDataset,\n",
    "    'movielens': MovieLensDataset,\n",
    "    'places365': Places365Dataset,\n",
    "    'random_gaussian': RandomGaussianImageDataset,\n",
    "    'random_rademacher': RandomRademacherImageDataset,\n",
    "    'speech_commands': SpeechCommandsDataset,\n",
    "    'svhn_cropped': SvhnDataset,\n",
    "    'glue/cola': GlueDatasets['glue/cola'],\n",
    "    'glue/sst2': GlueDatasets['glue/sst2'],\n",
    "    'glue/mrpc': GlueDatasets['glue/mrpc'],\n",
    "    'glue/qqp': GlueDatasets['glue/qqp'],\n",
    "    'glue/qnli': GlueDatasets['glue/qnli'],\n",
    "    'glue/rte': GlueDatasets['glue/rte'],\n",
    "    'glue/wnli': GlueDatasets['glue/wnli'],\n",
    "    'glue/stsb': GlueDatasets['glue/stsb'],\n",
    "    'wikipedia_toxicity': WikipediaToxicityDataset,\n",
    "    'genomics_ood': GenomicsOodDataset,\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset_names() -> List[str]:\n",
    "  return list(DATASETS.keys())\n",
    "\n",
    "\n",
    "def get(\n",
    "    dataset_name: str,\n",
    "    split: Union[Tuple[str, float], str, tfds.Split],\n",
    "    **hyperparameters: Any) -> BaseDataset:\n",
    "  \"\"\"Gets a dataset builder by name.\n",
    "  Note that the user still needs to call\n",
    "  `distribution_strategy.experimental_distribute_dataset(dataset)` on the loaded\n",
    "  dataset if they are running in a distributed environment.\n",
    "  Args:\n",
    "    dataset_name: Name of the dataset builder class.\n",
    "    split: a dataset split, either a custom tfds.Split or one of the\n",
    "      tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n",
    "      names.\n",
    "    **hyperparameters: dict of possible kwargs to be passed to the dataset\n",
    "      constructor.\n",
    "  Returns:\n",
    "    A dataset builder class with a method .build(split) which can be called to\n",
    "    get the tf.data.Dataset, which has elements that are a dict described by\n",
    "    dataset_builder.info.\n",
    "  Raises:\n",
    "    ValueError: If dataset_name is unrecognized.\n",
    "  \"\"\"\n",
    "  hyperparameters_py = {\n",
    "      k: (v.numpy().tolist() if isinstance(v, tf.Tensor) else v)\n",
    "      for k, v in hyperparameters.items()\n",
    "  }\n",
    "  logging.info(\n",
    "      'Building dataset %s with additional kwargs:\\n%s',\n",
    "      dataset_name,\n",
    "      json.dumps(hyperparameters_py, indent=2, sort_keys=True))\n",
    "  if dataset_name not in DATASETS:\n",
    "    raise ValueError('Unrecognized dataset name: {!r}'.format(dataset_name))\n",
    "\n",
    "  dataset_class = DATASETS[dataset_name]\n",
    "  return dataset_class(\n",
    "      split=split,\n",
    "      **hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
